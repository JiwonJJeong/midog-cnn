{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d229856-d584-4f3d-8dd1-8d33a1b3f77a",
   "metadata": {},
   "source": [
    "Start with baseline CNN inspired by LeNet-5. For this initial investigation, I want to visualize the improvements without considering any domain shifts. I will use the Canine Lymphoma, 3D Histech, VMU Vienna dataset (because it is the largest annotation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40061a2b-a539-4748-8622-d9bb502b4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from Machine Learning Engineering (Cornell Tech 2025)\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de6e9ad-7c6d-47c6-bef5-32a7a161053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying filters: {'Tumor': 'canine lymphoma', 'Scanner': '3D Histech', 'Origin': 'VMU Vienna'}\n",
      "Original size: 26286. Filtered size: 8216.\n",
      "\n",
      "--- Data Split Summary ---\n",
      "Original Total Annotations: 8216\n",
      "Train Annotations: 5750 (70.0%)\n",
      "Validation Annotations: 1233 (15.0%)\n",
      "Test Annotations: 1233 (15.0%)\n",
      "\n",
      "Train/Validation/Test DataLoaders created successfully.\n",
      "Train Loader batch size: 32\n",
      "Train Loader output shape: [Batch_Size, 3, 50, 50] (Verification)\n",
      "\n",
      "Verification Batch 1:\n",
      "  Image Batch Shape: torch.Size([32, 3, 60, 60])\n",
      "  Labels in Batch (first 5): tensor([0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from loaders import create_loaders\n",
    "\n",
    "try:\n",
    "    df_master = pd.read_csv('processed_annotations_with_patch_id.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Please run the preprocess_and_save_patches script first to create the data file.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Define your desired filter\n",
    "custom_filter = {\n",
    "    'Tumor': 'canine lymphoma',\n",
    "    'Scanner': '3D Histech',\n",
    "    'Origin': 'VMU Vienna'\n",
    "}\n",
    "\n",
    "# 3. Create loaders using the master DF\n",
    "train_loader, val_loader, test_loader = create_loaders(\n",
    "    df_master, \n",
    "    filters=custom_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6258b2e9-8166-42ab-b525-2b5089e1c6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 60, 60])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a79524c-533d-491d-a5d2-91f5fe3f9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Helper function to compute accuracy for a single data loader\n",
    "def _compute_accuracy(model: nn.Module, dataloader: torch.utils.data.DataLoader) -> Tuple[int, int]:\n",
    "    \"\"\"Computes total correct predictions and total samples for a given dataloader.\"\"\"\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Robust Batch Unpacking: Try to handle common formats\n",
    "        if isinstance(batch, dict):\n",
    "            # Assuming dictionary with 'image' and 'label' keys\n",
    "            X_batch = batch.get('image')\n",
    "            y_batch = batch.get('label')\n",
    "            if X_batch is None or y_batch is None:\n",
    "                 raise ValueError(\"DataLoader yields a dict, but 'image' or 'label' keys are missing.\")\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            # Assuming standard tuple (input, target). \n",
    "            # We enforce X_batch to be the first element, regardless of its content type for now.\n",
    "            X_batch, y_batch = batch[0], batch[1]\n",
    "        else:\n",
    "             raise TypeError(f\"Unexpected batch type: {type(batch)}. Expected tuple, list, or dict.\")\n",
    "\n",
    "        # CRITICAL CHECK: Ensure the input to the model is a Tensor\n",
    "        if not isinstance(X_batch, torch.Tensor):\n",
    "             print(f\"--- DEBUG: Batch input type is {type(X_batch)}. Expected torch.Tensor. ---\")\n",
    "             # If X_batch is a string (e.g., a path), the model will crash.\n",
    "             # This confirms the source of the TypeError.\n",
    "             raise TypeError(f\"Input to model is not a Tensor. Type received: {type(X_batch)}.\")\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, axis=1)\n",
    "        \n",
    "        total_correct += (predicted == y_batch).sum().item()\n",
    "        total_samples += y_batch.shape[0]\n",
    "        \n",
    "    return total_correct, total_samples\n",
    "\n",
    "def evaluate(model: nn.Module, trainloader: torch.utils.data.DataLoader, testloader: torch.utils.data.DataLoader) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the final training and test accuracies.\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_correct_train, total_samples_train = _compute_accuracy(model, trainloader)\n",
    "        total_correct_test, total_samples_test = _compute_accuracy(model, testloader)\n",
    "    \n",
    "    final_train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
    "    final_test_accuracy = total_correct_test / total_samples_test if total_samples_test > 0 else 0.0\n",
    "\n",
    "    return final_train_accuracy, final_test_accuracy\n",
    "\n",
    "# \n",
    "\n",
    "def plot_learning_curves(epoch_points: List[int], train_acc_points: List[float], test_acc_points: List[float]):\n",
    "    \"\"\"Helper function to create the accuracy plot.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epoch_points, train_acc_points, label='Training Accuracy', marker='o')\n",
    "    plt.plot(epoch_points, test_acc_points, label='Test Accuracy', marker='x')\n",
    "\n",
    "    plt.title('Learning Curves: Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_CNN(model: nn.Module, trainloader: torch.utils.data.DataLoader, testloader: torch.utils.data.DataLoader, num_epochs: int = 50, plot_interval: int = 10, lr: float = 0.01) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Trains a CNN model, evaluates accuracy periodically, and plots the learning curves.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    epoch_points = []\n",
    "    train_acc_points = []\n",
    "    test_acc_points = []\n",
    "    \n",
    "    # ----------------- Initial Evaluation (Epoch 0) -----------------\n",
    "    model.eval() \n",
    "    train_acc, test_acc = evaluate(model, trainloader, testloader) \n",
    "    \n",
    "    epoch_points.append(0)\n",
    "    train_acc_points.append(train_acc)\n",
    "    test_acc_points.append(test_acc)\n",
    "    \n",
    "    if plot_interval > 0:\n",
    "        print(f\"Epoch 0 (Initial): Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "\n",
    "    # ----------------- TRAINING LOOP -----------------\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        model.train() \n",
    "        \n",
    "        for batch in trainloader:\n",
    "            # Robust Batch Unpacking\n",
    "            if isinstance(batch, dict):\n",
    "                X_batch = batch.get('image')\n",
    "                y_batch = batch.get('label')\n",
    "            elif isinstance(batch, (list, tuple)):\n",
    "                X_batch, y_batch = batch[0], batch[1]\n",
    "            else:\n",
    "                 raise TypeError(f\"Unexpected batch type yielded by trainloader: {type(batch)}.\")\n",
    "\n",
    "            # CRITICAL CHECK: Input must be a Tensor\n",
    "            if not isinstance(X_batch, torch.Tensor):\n",
    "                 raise TypeError(f\"Input image data is not a Tensor. Type received: {type(X_batch)}. \"\n",
    "                                 \"Check your DataLoader's collate_fn or Dataset's __getitem__ method.\")\n",
    "            \n",
    "            # --- Standard Training Steps ---\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # ----------------- EVALUATION & STORAGE -----------------\n",
    "        if (plot_interval != 0 and epoch % plot_interval == 0) or epoch == num_epochs:\n",
    "            model.eval()\n",
    "            train_acc, test_acc = evaluate(model, trainloader, testloader) \n",
    "            \n",
    "            epoch_points.append(epoch)\n",
    "            train_acc_points.append(train_acc)\n",
    "            test_acc_points.append(test_acc)\n",
    "            \n",
    "            if plot_interval > 0:\n",
    "                print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "            \n",
    "    # ----------------- FINAL PLOT -----------------\n",
    "    if plot_interval > 0:\n",
    "        plot_learning_curves(epoch_points, train_acc_points, test_acc_points)\n",
    "    \n",
    "    print(f\"\\nTraining Complete. Final Train Acc: {train_acc:.4f}, Final Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d579db08-2fc4-4b2f-9ee2-762e841fdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cda14ac-a50e-4da5-bf26-80c0fbdb9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.LazyLinear(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.LazyLinear(2),\n",
    "        )\n",
    "        self.model.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "701ef38c-0666-4716-a075-15033c11074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Initial): Train Acc = 0.4819, Test Acc = 0.4818\n",
      "Epoch 10: Train Acc = 0.5181, Test Acc = 0.5182\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m lenet5 = lenet5()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_CNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlenet5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mtrain_CNN\u001b[39m\u001b[34m(model, trainloader, testloader, num_epochs, plot_interval, lr)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m     98\u001b[39m     model.train() \n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Robust Batch Unpacking\u001b[39;49;00m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cornell-tech/CT-applied-machine-learning/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cornell-tech/CT-applied-machine-learning/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cornell-tech/CT-applied-machine-learning/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/wsl_projects/midog-cnn/loaders.py:173\u001b[39m, in \u001b[36mCustomSingleAnnotationDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    169\u001b[39m patch_name = os.path.join(\u001b[38;5;28mself\u001b[39m.patch_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# 1. Load the small, pre-cropped patch directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     cropped_image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_name\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCropped patch not found at path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run preprocessing first!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cornell-tech/CT-applied-machine-learning/venv/lib/python3.12/site-packages/PIL/Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lenet5 = lenet5()\n",
    "train_CNN(lenet5, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d42eb9-ebb3-44a7-8355-7117efce3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alexnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        )\n",
    "        self.model.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c44dd-a201-48b2-8f48-87cb8d4a9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = alexnet()\n",
    "train_CNN(lenet5, train_loader, val_loader, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
