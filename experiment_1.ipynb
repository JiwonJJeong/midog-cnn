{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d229856-d584-4f3d-8dd1-8d33a1b3f77a",
      "metadata": {
        "id": "1d229856-d584-4f3d-8dd1-8d33a1b3f77a"
      },
      "source": [
        "Start with baseline CNN inspired by LeNet-5. For this initial investigation, I want to visualize the improvements without considering any domain shifts. I will use the Canine Lymphoma, 3D Histech, VMU Vienna dataset (because it is the largest annotation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "40061a2b-a539-4748-8622-d9bb502b4f38",
      "metadata": {
        "id": "40061a2b-a539-4748-8622-d9bb502b4f38"
      },
      "outputs": [],
      "source": [
        "# Code adapted from Machine Learning Engineering (Cornell Tech 2025)\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Path Definition ---\n",
        "# This path was verified to contain your loaders.py and your CSV\n",
        "PROJECT_DIR = '/content/drive/MyDrive/GoogleColab/dataformidogcnn'\n",
        "\n",
        "# Add the directory to the system path so Python can find 'loaders.py'\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.append(PROJECT_DIR)\n",
        "    print(f\"✅ Added {PROJECT_DIR} to Python system path.\")\n",
        "else:\n",
        "    print(\"Project directory already in system path.\")"
      ],
      "metadata": {
        "id": "xOOfsxbgvfJD",
        "outputId": "a923a7ae-3d1c-4d51-f914-b046c2d12791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xOOfsxbgvfJD",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Added /content/drive/MyDrive/GoogleColab/dataformidogcnn to Python system path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from loaders import create_loaders # This should now work\n",
        "\n",
        "# Define the full path to your CSV file\n",
        "CSV_PATH = os.path.join(PROJECT_DIR, 'processed_annotations_with_patch_id.csv')\n",
        "\n",
        "# Load the DataFrame and create the df_master variable\n",
        "try:\n",
        "    df_master = pd.read_csv(CSV_PATH)\n",
        "    print(f\"✅ df_master loaded successfully. Total rows: {len(df_master)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ CRITICAL ERROR: File not found at {CSV_PATH}. Execution stopping here.\")\n",
        "    df_master = None # Prevent NameError in next cell if failed"
      ],
      "metadata": {
        "id": "1HPIvs9x6yWP",
        "outputId": "b1e22e7d-e091-4667-ac49-c83422e8ebbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1HPIvs9x6yWP",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ df_master loaded successfully. Total rows: 26286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2de6e9ad-7c6d-47c6-bef5-32a7a161053c",
      "metadata": {
        "id": "2de6e9ad-7c6d-47c6-bef5-32a7a161053c",
        "outputId": "5e422cc6-e8e2-488b-a942-77547d020fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying filters: {'Tumor': 'canine lymphoma', 'Scanner': '3D Histech', 'Origin': 'VMU Vienna'}\n",
            "Original size: 26286. Filtered size: 8216.\n",
            "\n",
            "--- Data Split Summary ---\n",
            "Original Total Annotations: 8216\n",
            "Train Annotations: 5750 (70.0%)\n",
            "Validation Annotations: 1233 (15.0%)\n",
            "Test Annotations: 1233 (15.0%)\n",
            "\n",
            "Train/Validation/Test DataLoaders created successfully.\n",
            "Train Loader batch size: 32\n",
            "Train Loader output shape: [Batch_Size, 3, 50, 50] (Verification)\n",
            "\n",
            "Verification Batch 1:\n",
            "  Image Batch Shape: torch.Size([32, 3, 60, 60])\n",
            "  Labels in Batch (first 5): tensor([0, 0, 1, 0, 0])\n",
            "✅ DataLoaders created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Assuming your cropped_images folder is in the same directory as loaders.py and your CSV\n",
        "PROJECT_DIR = '/content/drive/MyDrive/GoogleColab/dataformidogcnn/'\n",
        "\n",
        "\n",
        "if 'df_master' in locals() and df_master is not None:\n",
        "\n",
        "    # Define custom_filter if you have it, or set to None\n",
        "    # Assuming custom_filter is not defined elsewhere, defining it here:\n",
        "    custom_filter = {\n",
        "      'Tumor': 'canine lymphoma',\n",
        "      'Scanner': '3D Histech',\n",
        "      'Origin': 'VMU Vienna'\n",
        "    }\n",
        "\n",
        "    # 3. Create loaders using the master DF\n",
        "    train_loader, val_loader, test_loader = create_loaders(\n",
        "        df_master,\n",
        "        patch_dir=PROJECT_DIR,\n",
        "        filters=custom_filter\n",
        "    )\n",
        "\n",
        "    print(\"✅ DataLoaders created successfully.\")\n",
        "else:\n",
        "    print(\"❌ Cannot create DataLoaders: df_master was not loaded. Rerun previous cells.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6258b2e9-8166-42ab-b525-2b5089e1c6fe",
      "metadata": {
        "id": "6258b2e9-8166-42ab-b525-2b5089e1c6fe",
        "outputId": "36ba3769-28d5-4b9e-e42f-4ea5a548fa1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 60, 60])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch['image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4a79524c-533d-491d-a5d2-91f5fe3f9c87",
      "metadata": {
        "id": "4a79524c-533d-491d-a5d2-91f5fe3f9c87"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List\n",
        "\n",
        "# Helper function to compute accuracy for a single data loader\n",
        "def _compute_accuracy(model: nn.Module, dataloader: torch.utils.data.DataLoader) -> Tuple[int, int]:\n",
        "    \"\"\"Computes total correct predictions and total samples for a given dataloader.\"\"\"\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Robust Batch Unpacking: Try to handle common formats\n",
        "        if isinstance(batch, dict):\n",
        "            # Assuming dictionary with 'image' and 'label' keys\n",
        "            X_batch = batch.get('image')\n",
        "            y_batch = batch.get('label')\n",
        "            if X_batch is None or y_batch is None:\n",
        "                 raise ValueError(\"DataLoader yields a dict, but 'image' or 'label' keys are missing.\")\n",
        "        elif isinstance(batch, (list, tuple)):\n",
        "            # Assuming standard tuple (input, target).\n",
        "            # We enforce X_batch to be the first element, regardless of its content type for now.\n",
        "            X_batch, y_batch = batch[0], batch[1]\n",
        "        else:\n",
        "             raise TypeError(f\"Unexpected batch type: {type(batch)}. Expected tuple, list, or dict.\")\n",
        "\n",
        "        # CRITICAL CHECK: Ensure the input to the model is a Tensor\n",
        "        if not isinstance(X_batch, torch.Tensor):\n",
        "             print(f\"--- DEBUG: Batch input type is {type(X_batch)}. Expected torch.Tensor. ---\")\n",
        "             # If X_batch is a string (e.g., a path), the model will crash.\n",
        "             # This confirms the source of the TypeError.\n",
        "             raise TypeError(f\"Input to model is not a Tensor. Type received: {type(X_batch)}.\")\n",
        "\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs, axis=1)\n",
        "\n",
        "        total_correct += (predicted == y_batch).sum().item()\n",
        "        total_samples += y_batch.shape[0]\n",
        "\n",
        "    return total_correct, total_samples\n",
        "\n",
        "def evaluate(model: nn.Module, trainloader: torch.utils.data.DataLoader, testloader: torch.utils.data.DataLoader) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Calculates the final training and test accuracies.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_correct_train, total_samples_train = _compute_accuracy(model, trainloader)\n",
        "        total_correct_test, total_samples_test = _compute_accuracy(model, testloader)\n",
        "\n",
        "    final_train_accuracy = total_correct_train / total_samples_train if total_samples_train > 0 else 0.0\n",
        "    final_test_accuracy = total_correct_test / total_samples_test if total_samples_test > 0 else 0.0\n",
        "\n",
        "    return final_train_accuracy, final_test_accuracy\n",
        "\n",
        "#\n",
        "\n",
        "def plot_learning_curves(epoch_points: List[int], train_acc_points: List[float], test_acc_points: List[float]):\n",
        "    \"\"\"Helper function to create the accuracy plot.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epoch_points, train_acc_points, label='Training Accuracy', marker='o')\n",
        "    plt.plot(epoch_points, test_acc_points, label='Test Accuracy', marker='x')\n",
        "\n",
        "    plt.title('Learning Curves: Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def train_CNN(model: nn.Module, trainloader: torch.utils.data.DataLoader, testloader: torch.utils.data.DataLoader, num_epochs: int = 50, plot_interval: int = 10, lr: float = 0.01) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Trains a CNN model, evaluates accuracy periodically, and plots the learning curves.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    epoch_points = []\n",
        "    train_acc_points = []\n",
        "    test_acc_points = []\n",
        "\n",
        "    # ----------------- Initial Evaluation (Epoch 0) -----------------\n",
        "    model.eval()\n",
        "    train_acc, test_acc = evaluate(model, trainloader, testloader)\n",
        "\n",
        "    epoch_points.append(0)\n",
        "    train_acc_points.append(train_acc)\n",
        "    test_acc_points.append(test_acc)\n",
        "\n",
        "    if plot_interval > 0:\n",
        "        print(f\"Epoch 0 (Initial): Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "\n",
        "    # ----------------- TRAINING LOOP -----------------\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for batch in trainloader:\n",
        "            # Robust Batch Unpacking\n",
        "            if isinstance(batch, dict):\n",
        "                X_batch = batch.get('image')\n",
        "                y_batch = batch.get('label')\n",
        "            elif isinstance(batch, (list, tuple)):\n",
        "                X_batch, y_batch = batch[0], batch[1]\n",
        "            else:\n",
        "                 raise TypeError(f\"Unexpected batch type yielded by trainloader: {type(batch)}.\")\n",
        "\n",
        "            # CRITICAL CHECK: Input must be a Tensor\n",
        "            if not isinstance(X_batch, torch.Tensor):\n",
        "                 raise TypeError(f\"Input image data is not a Tensor. Type received: {type(X_batch)}. \"\n",
        "                                 \"Check your DataLoader's collate_fn or Dataset's __getitem__ method.\")\n",
        "\n",
        "            # --- Standard Training Steps ---\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # ----------------- EVALUATION & STORAGE -----------------\n",
        "        if (plot_interval != 0 and epoch % plot_interval == 0) or epoch == num_epochs:\n",
        "            model.eval()\n",
        "            train_acc, test_acc = evaluate(model, trainloader, testloader)\n",
        "\n",
        "            epoch_points.append(epoch)\n",
        "            train_acc_points.append(train_acc)\n",
        "            test_acc_points.append(test_acc)\n",
        "\n",
        "            if plot_interval > 0:\n",
        "                print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "\n",
        "    # ----------------- FINAL PLOT -----------------\n",
        "    if plot_interval > 0:\n",
        "        plot_learning_curves(epoch_points, train_acc_points, test_acc_points)\n",
        "\n",
        "    print(f\"\\nTraining Complete. Final Train Acc: {train_acc:.4f}, Final Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    return train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d579db08-2fc4-4b2f-9ee2-762e841fdb0d",
      "metadata": {
        "id": "d579db08-2fc4-4b2f-9ee2-762e841fdb0d"
      },
      "outputs": [],
      "source": [
        "def init_weights(module):\n",
        "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
        "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
        "        nn.init.xavier_uniform_(module.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2cda14ac-a50e-4da5-bf26-80c0fbdb9ad6",
      "metadata": {
        "id": "2cda14ac-a50e-4da5-bf26-80c0fbdb9ad6"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class lenet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.Sigmoid(),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.Sigmoid(),\n",
        "            nn.LazyLinear(2),\n",
        "        )\n",
        "        self.model.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = {}"
      ],
      "metadata": {
        "id": "DnL1sjNA3e1h"
      },
      "id": "DnL1sjNA3e1h",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701ef38c-0666-4716-a075-15033c11074d",
      "metadata": {
        "id": "701ef38c-0666-4716-a075-15033c11074d"
      },
      "outputs": [],
      "source": [
        "model = lenet5()\n",
        "trainacc, valacc = train_CNN(model, train_loader, val_loader)\n",
        "accuracies[\"lenet5\"] = (trainacc, valacc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d42eb9-ebb3-44a7-8355-7117efce3d3f",
      "metadata": {
        "id": "01d42eb9-ebb3-44a7-8355-7117efce3d3f"
      },
      "outputs": [],
      "source": [
        "class alexnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(2)\n",
        "            )\n",
        "        self.model.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36c44dd-a201-48b2-8f48-87cb8d4a9ee1",
      "metadata": {
        "id": "f36c44dd-a201-48b2-8f48-87cb8d4a9ee1"
      },
      "outputs": [],
      "source": [
        "model = alexnet()\n",
        "trainacc, valacc = train_CNN(model, train_loader, val_loader)\n",
        "accuracies[\"alexnet\"] = (trainacc, valacc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class lenet5_relu_max_dropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.ReLU(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.ReLU(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(2),\n",
        "        )\n",
        "        self.model.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "ES40mIuyz1oq"
      },
      "id": "ES40mIuyz1oq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lenet5_relu_max_dropout()\n",
        "trainacc, valacc = train_CNN(model, train_loader, val_loader)\n",
        "accuracies[\"lenet5_relu_max_dropout\"] = (trainacc, valacc)"
      ],
      "metadata": {
        "id": "cYaQIZI90ow-"
      },
      "id": "cYaQIZI90ow-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class lenet5_relu(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(2),\n",
        "        )\n",
        "        self.model.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Azx78EQq4hfL"
      },
      "id": "Azx78EQq4hfL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lenet5_relu()\n",
        "trainacc, valacc = train_CNN(model, train_loader, val_loader)\n",
        "accuracies[\"lenet5_relu\"] = (trainacc, valacc)"
      ],
      "metadata": {
        "id": "zfZaP_iD9sGr"
      },
      "id": "zfZaP_iD9sGr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class lenet5_dropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.Sigmoid(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.Sigmoid(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(2),\n",
        "        )\n",
        "        self.model.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Ab_JzRTO4sBl"
      },
      "id": "Ab_JzRTO4sBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lenet5_dropout()\n",
        "trainacc, valacc = train_CNN(model, train_loader, val_loader)\n",
        "accuracies[\"lenet5_dropout\"] = (trainacc, valacc)"
      ],
      "metadata": {
        "id": "_EyyJA6H9wZU"
      },
      "id": "_EyyJA6H9wZU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class lenet5_max(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.Sigmoid(),\n",
        "            nn.LazyLinear(84),\n",
        "            nn.Sigmoid(),\n",
        "            nn.LazyLinear(2),\n",
        "        )\n",
        "        self.model.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "_56g-WXe93xM"
      },
      "id": "_56g-WXe93xM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lenet5_max()\n",
        "trainacc, valacc = train_CNN(model, train_loader, val_loader)\n",
        "accuracies[\"lenet5_max\"] = (trainacc, valacc)"
      ],
      "metadata": {
        "id": "avbcFWQN9-Dn"
      },
      "id": "avbcFWQN9-Dn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class lenet5_deep(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.Sigmoid(),\n",
        "            nn.LazyLinear(4096), nn.Sigmoid(),\n",
        "            nn.LazyLinear(2)\n",
        "            )\n",
        "        self.model.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "u8m4tAS-0MoM"
      },
      "id": "u8m4tAS-0MoM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lenet5_deep()\n",
        "trainacc, valacc = train_CNN(model, train_loader, val_loader)\n",
        "accuracies[model] = (trainacc, valacc)"
      ],
      "metadata": {
        "id": "0nUondkG0pGq"
      },
      "id": "0nUondkG0pGq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (venv)",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}